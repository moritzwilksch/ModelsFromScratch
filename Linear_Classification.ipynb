{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    "This notebook implements a linear Support Vector Machine from Scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "df_train = pd.read_csv('data/SPECT.train',header=None)\n",
    "df_test = pd.read_csv('data/SPECT.test',header=None)\n",
    "\n",
    "train = df_train.values\n",
    "test = df_test.values\n",
    "\n",
    "y_train = train[:,0]\n",
    "X_train = train[:,1:]\n",
    "y_test = test[:,0]\n",
    "X_test = test[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hinge Loss Function\n",
    "\n",
    "Given predictions from a model `preds` and ground truth labels `y` $\\in \\left\\{-1, 1\\right\\} $, `loss` calculates the hinge loss, that is, $\\max\\left(0, 1-y \\cdot \\textbf{x}^T \\textbf{w}\\right)$.  \n",
    "Moreover, it calculates the gradient of the hinge loss with respect to $\\textbf{w}$. \n",
    "Recall that the hinge loss function with regularization is given as $\\textbf{L}(\\textbf{w}) = \\sum_{i=1}^n \\left[ \\max \\left(0, 1-y_i \\textbf{x}_i^T \\textbf{w}  \\right) + \\frac{\\lambda}{n} \\cdot \\textbf{w}^T\\textbf{w}\\right]$\n",
    "\n",
    "The gradient of the hinge loss w.r.t. $\\textbf{w}$ for *a single sample* $x_i$ therefore is:  \n",
    "$\\nabla_{x_i} \\textbf{L}(\\textbf{w}) = \\begin{cases}   \\frac{2\\lambda}{n} \\textbf{w} && \\text{if  } y_i \\textbf{x}_i^T \\textbf{w} > 1 \\\\ \\frac{2\\lambda}{n} \\textbf{w} - y_i \\textbf{x}_i && \\text{if  } y_i \\textbf{x}_i^T \\textbf{w} < 1     \\end{cases}$  \n",
    "\n",
    "**Note** however, that in this implementation the regularization is taken care of separately (in the `reg` function). Therefore, the gradient that is returend by `loss` **does not contain** the term $\\frac{2\\lambda}{n} \\textbf{w}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(preds: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calculates hinge loss on predictions preds and ground truth y\"\"\"\n",
    "    l = np.max(\n",
    "        np.hstack([\n",
    "            np.zeros((len(y), 1)),\n",
    "            (1-y*preds).reshape(-1, 1)\n",
    "        ]), axis=1\n",
    "    )\n",
    "\n",
    "    g = l.copy()\n",
    "    mask = g > 0\n",
    "    g[mask] = -y[mask]\n",
    "    g[~mask] = 0\n",
    "\n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([0., 1., 0., 5.]), array([ 0., -1.,  0.,  1.]))"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Example:\n",
    "loss(\n",
    "    preds = np.array([2, 0, -3, 4]),\n",
    "    y = np.array([1, 1, -1, -1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathcal{L}_2$ Regularizer\n",
    "\n",
    "Value of $\\mathcal{L}_2$-regularizer $r$ and its gradient $g$ at point $\\textbf{w}$. This is the separate regularization term which â€“ in this implementation - is not included in the loss function but handeled separately.\n",
    "\n",
    "\n",
    "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
    "\n",
    "$$g = \\lambda \\textbf{w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_reg(w: np.ndarray, lambda_: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns value of L2 regularizer and its gradient at point w.\"\"\"\n",
    "    r = lambda_ / 2 * np.dot(w.T, w)\n",
    "    g = lambda_ * w\n",
    "\n",
    "    return r, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `learn_model` function\n",
    "\n",
    "For a given $n\\times m$ design matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ `learn_model` learns the parameters $\\textbf{w}$ of a linear classification model.\n",
    "The class label has to be $\\in \\left \\{-1,1 \\right \\}$. \n",
    "The trade-off parameter between the empirical loss and the regularizer is $\\lambda > 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(X: np.ndarray, y: np.ndarray, lambda_: float, verbose: int = 0) -> np.ndarray:\n",
    "    \"\"\"Learns parameters w of a linear classification model on design matrix X with class labels y (either -1 or 1).\"\"\"\n",
    "    max_iter = 200\n",
    "    eps  = 0.001\n",
    "    alpha = 1\n",
    "\n",
    "    # Random initialization\n",
    "    w = np.random.randn(X.shape[1])  \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Calculate current predictions\n",
    "        curr_preds = np.dot(X, w)\n",
    "        # Calculate current loss & loss gradient\n",
    "        curr_loss, curr_loss_gradient = loss(curr_preds, y)\n",
    "        \n",
    "        if verbose >= 1:\n",
    "            print (f'loss: {np.mean(curr_loss)}')\n",
    "        \n",
    "        # Compute value of regularizer & regularizer gradient\n",
    "        regu, regu_gradient = l2_reg(w, lambda_)\n",
    "\n",
    "        # Calculate the whole gradient (including loss AND regularizer)\n",
    "        # As the loss gradient depends on/contains x, these are mutiplied,\n",
    "        # regu_gradient is added as it does not contain/depend on x.\n",
    "        gradient = np.dot(X.T, curr_loss_gradient) + regu_gradient \n",
    "\n",
    "        # Calculate step size. Alternatively: while L(w)_new > L(w): a /= 2\n",
    "        if (i > 0):\n",
    "            alpha = alpha * (\n",
    "                (np.dot(gradient_old.T, gradient_old))\n",
    "                /(np.dot((gradient_old - gradient).T, gradient_old))\n",
    "            )\n",
    "\n",
    "            if verbose >= 2:\n",
    "                print(f\"alpha = {alpha}\")\n",
    "\n",
    "        # Update weights\n",
    "        w = w - alpha * gradient\n",
    "\n",
    "        # Stop if improvement is marginal.\n",
    "        if (np.linalg.norm(alpha * gradient) < eps):\n",
    "            break\n",
    "\n",
    "        gradient_old = gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Predict Function\n",
    "\n",
    "Once the weights are learned, the weights and a design matrix $X$ can be passed to the predict function to make predictions. The output of the decision function $f_{\\textbf{w}}(\\textbf{x}) = \\textbf{x}^T \\textbf{w}$ is converted to a class label $\\in \\{-1, 1\\}$ using the $sign$ function. Note that this assumes that the bias term is included in $\\textbf{w}$ and an additional column of $1$s was appended to $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given already learned weights w, predicts class labels for design matrix X.\"\"\"\n",
    "    preds = np.matmul(X, w.T)\n",
    "    \n",
    "    # sign-function converts output of decision function to class label.\n",
    "    preds[preds > 0] = 1\n",
    "    preds[preds <= 0] = -1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss: 1.242184916850876\nloss: 14.603246301131836\nalpha = 0.09280655555218147\nloss: 1.009377189241246\nalpha = 0.09891724547921082\nloss: 1.3902623933638396\nalpha = 0.052192663612385524\nloss: 1.2260218229050577\nalpha = 0.03022791595572736\nloss: 0.8191647047596419\nalpha = 0.08225856902721411\nloss: 1.8341849144752829\nalpha = 0.024502363973239044\nloss: 0.7492270941674827\nalpha = 0.030394666608220152\nloss: 0.7528214205198218\nalpha = 0.014149500768185906\nloss: 0.725587807128435\nalpha = 0.02665963947547244\nloss: 0.7614814158533855\nalpha = 0.008303640046587466\nloss: 0.7203614807911649\nalpha = 0.00930299852488308\nloss: 0.7180861009791238\nalpha = 0.007134165440248881\nloss: 0.7202939112305451\nalpha = 0.005515693737464294\nloss: 0.7190644798880989\nalpha = 0.0033594981029090474\nloss: 0.7196854075819689\nalpha = 0.003516937803319199\nloss: 0.7194327012663961\nalpha = 0.09999999999999984\nloss: 0.7212500000000001\nalpha = 0.029539034551996125\nloss: 0.8189686866058776\nalpha = 0.008310989600355308\nloss: 0.7232214916375707\nalpha = 0.010020102432337547\nloss: 0.7173575668000536\nalpha = 0.005501045290735432\nloss: 0.7191065172954525\nalpha = 0.0037291410769844007\nloss: 0.7179246507046664\nalpha = 0.0027721563438942417\nloss: 0.7183678221160408\nalpha = 0.0032844793736243588\nloss: 0.7180960657804176\nalpha = 0.0029963148178365298\nloss: 0.7182903811168941\nalpha = 0.003802948334831467\nloss: 0.7180521632042847\nalpha = 0.0037350046959839286\nloss: 0.7189997292971887\nalpha = 0.002034549217394694\nloss: 0.718321125033014\nalpha = 0.0015906114649064306\nloss: 0.7180695200134453\nalpha = 0.0008683179909256237\nloss: 0.7181144135663399\nalpha = 0.0004943269914950554\nloss: 0.7180942391626495\nalpha = 0.005079857262378704\nloss: 0.7200312226030605\nalpha = 0.002509442674364977\nloss: 0.7187200130064055\nalpha = 0.01104568719891158\nloss: 0.7190359983159489\nalpha = 0.0038170152581361995\nloss: 0.719673508657717\nalpha = 0.004731658107378514\nloss: 0.7197936604902535\nalpha = 0.0031786770308598563\nloss: 0.7183365545200474\nalpha = 0.0018014756001964827\nloss: 0.7188311030967118\nalpha = 0.0010380753237100868\nloss: 0.7185129903104114\nalpha = 0.0008945998460051253\nloss: 0.7185889799522442\nalpha = 0.0008282508533297114\nloss: 0.7185385479238308\nalpha = 0.09999999999999973\nloss: 0.7212500000000001\nalpha = 0.01670231979703322\nloss: 0.7375828935281022\nalpha = 0.006797786773372945\nloss: 0.7193906344463443\nalpha = 0.005814147513244466\nloss: 0.7200782809905446\nalpha = 0.006295131161862204\nloss: 0.7186984600677587\nalpha = 0.004384290480295187\nloss: 0.7192606495811021\nalpha = 0.004576656729275376\nloss: 0.7188097913531035\nalpha = 0.020295610954472538\nloss: 0.7212953254805444\nalpha = 0.011497467940124724\nloss: 0.7195540593265177\nalpha = 0.004737924447807153\nloss: 0.7200746009093659\nalpha = 0.0032150508313343778\nloss: 0.7190734168486952\nalpha = 0.0025114607089422075\nloss: 0.7187146922882478\nalpha = 0.0026420012090181875\nloss: 0.7185505000428556\nalpha = 0.0999999999999998\nloss: 0.7212500000000001\nalpha = 0.017734700117133993\nloss: 0.7400819040560126\nalpha = 0.006574867864398829\nloss: 0.7190023573070151\nalpha = 0.006375113656967402\nloss: 0.7189862692418713\nalpha = 0.10000000000000002\nloss: 0.73625\nalpha = 0.03153827932440116\nloss: 0.7316825670355258\nalpha = 0.0136933171870873\nloss: 0.72513149242829\nalpha = 0.010147653642770035\nloss: 0.7220426441651914\nalpha = 0.005316484978846968\nloss: 0.72187184298149\nalpha = 0.003980322017981844\nloss: 0.7208938169231951\nalpha = 0.004973436761470008\nloss: 0.7204763557466464\nalpha = 0.10000000000000026\nloss: 0.7212500000000001\nalpha = 0.024089022114632007\nloss: 0.7684537879661818\nalpha = 0.008261868427512876\nloss: 0.7210882855748437\nalpha = 0.009625481090640195\nloss: 0.7195555139733678\nalpha = 0.004600025440437036\nloss: 0.720225221728717\nalpha = 0.004335651387571337\nloss: 0.719417095158524\nalpha = 0.0024316205748968196\nloss: 0.7196780480450263\nalpha = 0.0014510595582133913\nloss: 0.7190718361961704\nalpha = 0.0009414894103444903\nloss: 0.7191080472127059\nalpha = 0.0005901641323118975\nloss: 0.7189152716542755\nalpha = 0.0006723311315390024\nloss: 0.7187965370811578\nalpha = 0.0006326155380330565\nloss: 0.7188427118211137\nalpha = 0.0005244951598718466\nloss: 0.7187615041484605\nalpha = 0.0002743587889024252\nloss: 0.7187389128592199\nalpha = 0.0001570145318129452\n === TRAIN Set Performance ===\n[[16 24]\n [ 3 37]]\nROC AUC = 0.6625000000000001\n"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# Map class labels from {0, 1} to {-1, 1}\n",
    "y_train[y_train == 0] = -1\n",
    "\n",
    "# Learn weights and predict\n",
    "w = learn_model(X_train, y_train, lambda_=10, verbose=2)\n",
    "preds = predict(w, X_train)\n",
    "\n",
    "print(\" === TRAIN Set Performance ===\")\n",
    "print(confusion_matrix(y_train, preds))\n",
    "print(f\"ROC AUC = {roc_auc_score(y_train, preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=== TRAIN Set Performance ===\n[[  8   7]\n [ 11 161]]\nROC AUC = 0.7346899224806202\n"
    }
   ],
   "source": [
    "# Map class labels from {0, 1} to {-1, 1}\n",
    "y_test[y_test == 0] = -1\n",
    "preds = predict(w, X_test)\n",
    "\n",
    "print(\" === TRAIN Set Performance ===\")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(f\"ROC AUC = {roc_auc_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with Vanilla Random Forsest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=== TRAIN Set Performance ===\n[[40  0]\n [ 5 35]]\nROC AUC = 0.9375\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_train)\n",
    "\n",
    "print(\"=== TRAIN Set Performance ===\")\n",
    "print(confusion_matrix(y_train, preds))\n",
    "print(f\"ROC AUC = {roc_auc_score(y_train, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=== TEST Set Performance ===\n[[ 12   3]\n [ 39 133]]\nROC AUC = 0.7866279069767442\n"
    }
   ],
   "source": [
    "print(\"=== TEST Set Performance ===\")\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(f\"ROC AUC = {roc_auc_score(y_test, preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}